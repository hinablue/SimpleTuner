#!/usr/bin/env python3
"""
AdamWBF16_HinaAdaptive å„ªåŒ–å™¨æ¸¬è©¦è…³æœ¬

é€™å€‹è…³æœ¬ç”¨æ–¼æ¸¬è©¦å„ªåŒ–å™¨çš„åŸºæœ¬åŠŸèƒ½å’Œæ­£ç¢ºæ€§ã€‚
"""

import torch
import torch.nn as nn
import sys
import os

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ°è·¯å¾‘
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from adamw_bf16_hinaadaptive import AdamWBF16_HinaAdaptive


class TestModel(nn.Module):
    """ç”¨æ–¼æ¸¬è©¦çš„ç°¡å–®æ¨¡å‹"""
    def __init__(self, input_size=128, hidden_size=256, output_size=64):
        super().__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, hidden_size)
        self.linear3 = nn.Linear(hidden_size, output_size)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.linear1(x))
        x = self.relu(self.linear2(x))
        x = self.linear3(x)
        return x


def test_basic_functionality():
    """æ¸¬è©¦åŸºæœ¬åŠŸèƒ½"""
    print("æ¸¬è©¦ 1: åŸºæœ¬åŠŸèƒ½æ¸¬è©¦")

    # æª¢æŸ¥ CUDA å¯ç”¨æ€§
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è£ç½®: {device}")

    # å‰µå»ºæ¨¡å‹ä¸¦è½‰æ›ç‚º bfloat16
    model = TestModel().to(device).to(torch.bfloat16)

    # å‰µå»ºå„ªåŒ–å™¨
    optimizer = AdamWBF16_HinaAdaptive(model.parameters(), lr=1e-3)

    # å‰µå»ºè™›æ“¬æ•¸æ“š
    batch_size = 8
    input_data = torch.randn(batch_size, 128, device=device, dtype=torch.bfloat16)
    target_data = torch.randn(batch_size, 64, device=device, dtype=torch.bfloat16)

    # æå¤±å‡½æ•¸
    criterion = nn.MSELoss()

    print("åŸ·è¡Œå‰å‘å’Œå¾Œå‘å‚³æ’­...")

    # å‰å‘å‚³æ’­
    optimizer.zero_grad()
    output = model(input_data)
    loss = criterion(output, target_data)

    print(f"åˆå§‹æå¤±: {loss.item():.6f}")

    # å¾Œå‘å‚³æ’­
    loss.backward()

    # æª¢æŸ¥æ¢¯åº¦
    grad_norm = 0.0
    for param in model.parameters():
        if param.grad is not None:
            grad_norm += param.grad.norm().item() ** 2
    grad_norm = grad_norm ** 0.5

    print(f"æ¢¯åº¦ç¯„æ•¸: {grad_norm:.6f}")

    # å„ªåŒ–æ­¥é©Ÿ
    optimizer.step()

    # å†æ¬¡å‰å‘å‚³æ’­æª¢æŸ¥æå¤±è®ŠåŒ–
    with torch.no_grad():
        new_output = model(input_data)
        new_loss = criterion(new_output, target_data)

    print(f"å„ªåŒ–å¾Œæå¤±: {new_loss.item():.6f}")
    print(f"æå¤±è®ŠåŒ–: {new_loss.item() - loss.item():.6f}")

    print("âœ… åŸºæœ¬åŠŸèƒ½æ¸¬è©¦é€šé\n")


def test_adaptive_features():
    """æ¸¬è©¦è‡ªé©æ‡‰åŠŸèƒ½"""
    print("æ¸¬è©¦ 2: è‡ªé©æ‡‰åŠŸèƒ½æ¸¬è©¦")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = TestModel().to(device).to(torch.bfloat16)

    # å•Ÿç”¨æ‰€æœ‰è‡ªé©æ‡‰åŠŸèƒ½
    optimizer = AdamWBF16_HinaAdaptive(
        model.parameters(),
        lr=1e-3,
        use_dynamic_adaptation=True,
        relationship_discovery_interval=5,  # è¨­å®šè¼ƒçŸ­é–“éš”ç”¨æ–¼æ¸¬è©¦
        adaptation_strength=1.2,
        compatibility_threshold=0.2
    )

    # å‰µå»ºæ•¸æ“š
    input_data = torch.randn(4, 128, device=device, dtype=torch.bfloat16)
    target_data = torch.randn(4, 64, device=device, dtype=torch.bfloat16)
    criterion = nn.MSELoss()

    print("åŸ·è¡Œå¤šæ­¥è¨“ç·´ä»¥è§¸ç™¼è‡ªé©æ‡‰åŠŸèƒ½...")

    for step in range(10):
        optimizer.zero_grad()
        output = model(input_data)
        loss = criterion(output, target_data)
        loss.backward()
        optimizer.step()

        if step % 3 == 0:
            print(f"æ­¥é©Ÿ {step+1}, æå¤±: {loss.item():.6f}")

    # æª¢æŸ¥å„ªåŒ–å™¨ç‹€æ…‹
    info = optimizer.get_optimization_info()
    print(f"ç¸½è¨“ç·´æ­¥æ•¸: {info.get('training_stats', {}).get('global_step', 'N/A')}")
    print(f"ç™¼ç¾çš„åƒæ•¸é—œä¿‚: {info.get('training_stats', {}).get('total_relationships', 'N/A')}")

    # é‡è¦æ€§åˆ†æ
    importance = optimizer.get_importance_analysis()
    if 'importance_statistics' in importance:
        stats = importance['importance_statistics']
        print(f"åƒæ•¸é‡è¦æ€§ - å¹³å‡: {stats['mean']:.4f}, æœ€å¤§: {stats['max']:.4f}")

    print("âœ… è‡ªé©æ‡‰åŠŸèƒ½æ¸¬è©¦é€šé\n")


def test_memory_optimization():
    """æ¸¬è©¦è¨˜æ†¶é«”å„ªåŒ–åŠŸèƒ½"""
    print("æ¸¬è©¦ 3: è¨˜æ†¶é«”å„ªåŒ–æ¸¬è©¦")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = TestModel().to(device).to(torch.bfloat16)

    optimizer = AdamWBF16_HinaAdaptive(
        model.parameters(),
        lr=1e-3,
        use_orthogonal_grad=True  # å•Ÿç”¨éœ€è¦ç·©è¡å€çš„åŠŸèƒ½
    )

    # åŸ·è¡Œå¹¾æ­¥è¨“ç·´ä¾†å‰µå»ºç·©è¡å€
    input_data = torch.randn(4, 128, device=device, dtype=torch.bfloat16)
    target_data = torch.randn(4, 64, device=device, dtype=torch.bfloat16)
    criterion = nn.MSELoss()

    for _ in range(5):
        optimizer.zero_grad()
        output = model(input_data)
        loss = criterion(output, target_data)
        loss.backward()
        optimizer.step()

    # æª¢æŸ¥ç·©è¡å€æ± çµ±è¨ˆ
    memory_stats = optimizer.get_buffer_pool_stats()
    if 'total_buffers' in memory_stats:
        print(f"ç·©è¡å€æ± çµ±è¨ˆ:")
        print(f"  ç¸½ç·©è¡å€æ•¸: {memory_stats['total_buffers']}")
        print(f"  ç·©è¡å€é¡å‹æ•¸: {memory_stats['total_buffer_types']}")
        print(f"  ä¼°è¨ˆè¨˜æ†¶é«”ä½¿ç”¨: {memory_stats['estimated_memory_mb']:.2f} MB")
    else:
        print("ç·©è¡å€æ± æœªåˆå§‹åŒ–æˆ–ç„¡ç·©è¡å€")

    # æ¸¬è©¦ç·©è¡å€æ¸…ç†
    optimizer.clear_buffer_pool()
    cleared_stats = optimizer.get_buffer_pool_stats()
    print(f"æ¸…ç†å¾Œç·©è¡å€æ•¸: {cleared_stats.get('total_buffers', 0)}")

    print("âœ… è¨˜æ†¶é«”å„ªåŒ–æ¸¬è©¦é€šé\n")


def test_advanced_features():
    """æ¸¬è©¦é€²éšå„ªåŒ–æŠ€è¡“"""
    print("æ¸¬è©¦ 4: é€²éšå„ªåŒ–æŠ€è¡“æ¸¬è©¦")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = TestModel().to(device).to(torch.bfloat16)

    # å•Ÿç”¨å„ç¨®é€²éšåŠŸèƒ½
    optimizer = AdamWBF16_HinaAdaptive(
        model.parameters(),
        lr=1e-3,
        use_spd=True,
        use_cautious=True,
        use_adopt_stability=True,
        use_agr=True,
        use_tam=True,
        dynamic_weight_decay=True
    )

    # å‰µå»ºæ•¸æ“šä¸¦è¨“ç·´
    input_data = torch.randn(4, 128, device=device, dtype=torch.bfloat16)
    target_data = torch.randn(4, 64, device=device, dtype=torch.bfloat16)
    criterion = nn.MSELoss()

    initial_loss = None

    for step in range(10):
        optimizer.zero_grad()
        output = model(input_data)
        loss = criterion(output, target_data)

        if initial_loss is None:
            initial_loss = loss.item()

        loss.backward()
        optimizer.step()

    final_loss = loss.item()
    improvement = initial_loss - final_loss

    print(f"åˆå§‹æå¤±: {initial_loss:.6f}")
    print(f"æœ€çµ‚æå¤±: {final_loss:.6f}")
    print(f"æ”¹å–„ç¨‹åº¦: {improvement:.6f}")

    # æª¢æŸ¥æ˜¯å¦æœ‰æ”¶æ–‚è¶¨å‹¢
    if improvement > 0:
        print("âœ… æ¨¡å‹é¡¯ç¤ºæ”¶æ–‚è¶¨å‹¢")
    else:
        print("âš ï¸ æ¨¡å‹æœªé¡¯ç¤ºæ˜é¡¯æ”¶æ–‚ï¼ˆå¯èƒ½éœ€è¦èª¿æ•´è¶…åƒæ•¸ï¼‰")

    print("âœ… é€²éšå„ªåŒ–æŠ€è¡“æ¸¬è©¦é€šé\n")


def test_error_handling():
    """æ¸¬è©¦éŒ¯èª¤è™•ç†"""
    print("æ¸¬è©¦ 5: éŒ¯èª¤è™•ç†æ¸¬è©¦")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # æ¸¬è©¦é bfloat16 åƒæ•¸çš„éŒ¯èª¤è™•ç†
    model_float32 = TestModel().to(device).to(torch.float32)
    optimizer = AdamWBF16_HinaAdaptive(model_float32.parameters(), lr=1e-3)

    input_data = torch.randn(4, 128, device=device, dtype=torch.float32)
    target_data = torch.randn(4, 64, device=device, dtype=torch.float32)
    criterion = nn.MSELoss()

    try:
        optimizer.zero_grad()
        output = model_float32(input_data)
        loss = criterion(output, target_data)
        loss.backward()
        optimizer.step()
        print("âŒ æ‡‰è©²æ‹‹å‡º bfloat16 é¡å‹éŒ¯èª¤")
    except AssertionError as e:
        print("âœ… æ­£ç¢ºæ•ç² bfloat16 é¡å‹éŒ¯èª¤")
    except Exception as e:
        print(f"âš ï¸ æ•ç²åˆ°æ„å¤–éŒ¯èª¤: {e}")

    print("âœ… éŒ¯èª¤è™•ç†æ¸¬è©¦å®Œæˆ\n")


def test_parameter_validation():
    """æ¸¬è©¦åƒæ•¸é©—è­‰"""
    print("æ¸¬è©¦ 6: åƒæ•¸é©—è­‰æ¸¬è©¦")

    model = TestModel()

    # æ¸¬è©¦ç„¡æ•ˆåƒæ•¸
    invalid_configs = [
        {"lr": -1e-3, "expected_error": "learning rate"},
        {"eps": -1e-8, "expected_error": "epsilon"},
        {"betas": (1.5, 0.999), "expected_error": "beta"},
        {"betas": (0.9, 1.5), "expected_error": "beta"},
        {"weight_decay": -0.01, "expected_error": "weight_decay"}
    ]

    passed_tests = 0

    for config in invalid_configs:
        try:
            test_config = {"lr": 1e-3, "betas": (0.9, 0.999), "eps": 1e-8, "weight_decay": 0.01}
            test_config.update({k: v for k, v in config.items() if k != "expected_error"})

            optimizer = AdamWBF16_HinaAdaptive(model.parameters(), **test_config)
            print(f"âŒ æ‡‰è©²æ‹‹å‡º {config['expected_error']} éŒ¯èª¤")
        except ValueError as e:
            if config["expected_error"].lower() in str(e).lower():
                print(f"âœ… æ­£ç¢ºæ•ç² {config['expected_error']} éŒ¯èª¤")
                passed_tests += 1
            else:
                print(f"âš ï¸ éŒ¯èª¤è¨Šæ¯ä¸ç¬¦åˆé æœŸ: {e}")
        except Exception as e:
            print(f"âš ï¸ æ•ç²åˆ°æ„å¤–éŒ¯èª¤é¡å‹: {type(e).__name__}: {e}")

    print(f"åƒæ•¸é©—è­‰æ¸¬è©¦: {passed_tests}/{len(invalid_configs)} é€šé\n")


def run_all_tests():
    """åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦"""
    print("=" * 60)
    print("AdamWBF16_HinaAdaptive å„ªåŒ–å™¨å®Œæ•´æ¸¬è©¦å¥—ä»¶")
    print("=" * 60)

    tests = [
        test_basic_functionality,
        test_adaptive_features,
        test_memory_optimization,
        test_advanced_features,
        test_error_handling,
        test_parameter_validation
    ]

    passed = 0
    total = len(tests)

    for i, test_func in enumerate(tests, 1):
        try:
            print(f"\n[{i}/{total}] åŸ·è¡Œ {test_func.__name__}...")
            test_func()
            passed += 1
        except Exception as e:
            print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()

    print("=" * 60)
    print(f"æ¸¬è©¦çµæœ: {passed}/{total} é€šé")

    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼å„ªåŒ–å™¨é‹è¡Œæ­£å¸¸ã€‚")
    else:
        print("âš ï¸ éƒ¨åˆ†æ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥å¯¦ç¾ã€‚")

    print("=" * 60)


if __name__ == "__main__":
    # æª¢æŸ¥ç’°å¢ƒ
    print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPU åç¨±: {torch.cuda.get_device_name()}")

    # æª¢æŸ¥ bfloat16 æ”¯æ´
    try:
        test_tensor = torch.tensor([1.0], dtype=torch.bfloat16)
        print("âœ… bfloat16 æ”¯æ´æ­£å¸¸")
    except Exception as e:
        print(f"âŒ bfloat16 æ”¯æ´æœ‰å•é¡Œ: {e}")

    # åŸ·è¡Œæ¸¬è©¦
    run_all_tests()